<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<title></title>
		<meta name='Generator' content='Zim 0.52'>
		<style type='text/css'>
			a          { text-decoration: none      }
			a:hover    { text-decoration: underline }
			a:active   { text-decoration: underline }
			strike     { color: grey                }
			u          { text-decoration: none;
			             background-color: yellow   }
			tt         { color: #2e3436;            }
			pre        { color: #2e3436;
			             margin-left: 20px          }
			h1         { text-decoration: underline;
			             color: #4e9a06             }
			h2         { color: #4e9a06             }
			h3         { color: #4e9a06             }
			h4         { color: #4e9a06             }
			h5         { color: #4e9a06             }
			span.insen { color: grey                }
		</style>
	</head>
	<body>

<!-- Header -->

	[ <a href='./curl.html'>Prev</a> ]

			[ <a href='../../index.html'>Index</a> ]

	[ <a href='../zzz_Undokumentiert.html'>Next</a> ]

<!-- End Header -->

<hr />

<!-- Wiki content -->

<h1>wget</h1>

<li>unterbrochene Downloads können zu einem späteren Zeitpunkt fortgesetzt werden</li>
<li>unterstützt die Protokolle FTP, HTTP HTTPS</li>
<a href="../Netzwerke/Protokolle.html" title="Linux Wissen:Netzwerke:Protokolle">Linux Wissen:Netzwerke:Protokolle</a>

<strong>wget &lt;URI&gt;|&lt;URL&gt;</strong><br>
→ läd eine Datei von der URI oder eine html Website von der URL herunter<br>
→ um Dateien komplett zum offline lesen herunterzuladen, muss <strong>-p </strong>verwendet werden<br>
<strong>-c </strong>→ abgebrochene Downloads von URL|URI wieder aufnehmen<br>
<strong>-t N </strong>→ abgebrochenen Downloads <strong>N </strong>mal wiederversuchen<br>
<strong>--retry-confused </strong>→ bei einem "connection refused" wird ein neuer Versuch gestartet<br>
<strong>-p </strong>→ läd zusätzlich zur Website CSS Stile, Bilder und andere zugehörige Dateien herunter<br>
<strong>-k </strong>→ die Links der Website wiederherstellen<br>
<strong>-E </strong>→ Scriptdateien (php, asp) in .html umwandeln<br>
<br>
<br>
<h4>Rekursive Optionen</h4>
<strong>-r</strong> → Links rekursiv verfolgen<br>
<strong>-l N</strong> → die Rekursionstiefe auf N festlegen<br>
<strong>-H </strong>→ verfolge Links auf externen Websites


<!-- End wiki content -->

<hr />

<!-- Backlinks -->

	Backlinks:		<a href='../Web_Fetch_Tools.html'>Linux Wissen:Web Fetch Tools</a></li>

<!-- End Backlinks -->

	</body>

</html>
